{
  
    
        "post0": {
            "title": "Basic Preprocessing for NLP",
            "content": "%reload_ext autoreload %autoreload 2 %matplotlib inline . import os import glob from io import open import pandas as pd import numpy as np import re . import nltk nltk.download(&quot;popular&quot;) . [nltk_data] Downloading collection &#39;popular&#39; [nltk_data] | [nltk_data] | Downloading package cmudict to /root/nltk_data... [nltk_data] | Package cmudict is already up-to-date! [nltk_data] | Downloading package gazetteers to /root/nltk_data... [nltk_data] | Package gazetteers is already up-to-date! [nltk_data] | Downloading package genesis to /root/nltk_data... [nltk_data] | Package genesis is already up-to-date! [nltk_data] | Downloading package gutenberg to /root/nltk_data... [nltk_data] | Package gutenberg is already up-to-date! [nltk_data] | Downloading package inaugural to /root/nltk_data... [nltk_data] | Package inaugural is already up-to-date! [nltk_data] | Downloading package movie_reviews to [nltk_data] | /root/nltk_data... [nltk_data] | Package movie_reviews is already up-to-date! [nltk_data] | Downloading package names to /root/nltk_data... [nltk_data] | Package names is already up-to-date! [nltk_data] | Downloading package shakespeare to /root/nltk_data... [nltk_data] | Package shakespeare is already up-to-date! [nltk_data] | Downloading package stopwords to /root/nltk_data... [nltk_data] | Package stopwords is already up-to-date! [nltk_data] | Downloading package treebank to /root/nltk_data... [nltk_data] | Package treebank is already up-to-date! [nltk_data] | Downloading package twitter_samples to [nltk_data] | /root/nltk_data... [nltk_data] | Package twitter_samples is already up-to-date! [nltk_data] | Downloading package omw to /root/nltk_data... [nltk_data] | Package omw is already up-to-date! [nltk_data] | Downloading package wordnet to /root/nltk_data... [nltk_data] | Package wordnet is already up-to-date! [nltk_data] | Downloading package wordnet_ic to /root/nltk_data... [nltk_data] | Package wordnet_ic is already up-to-date! [nltk_data] | Downloading package words to /root/nltk_data... [nltk_data] | Package words is already up-to-date! [nltk_data] | Downloading package maxent_ne_chunker to [nltk_data] | /root/nltk_data... [nltk_data] | Package maxent_ne_chunker is already up-to-date! [nltk_data] | Downloading package punkt to /root/nltk_data... [nltk_data] | Package punkt is already up-to-date! [nltk_data] | Downloading package snowball_data to [nltk_data] | /root/nltk_data... [nltk_data] | Package snowball_data is already up-to-date! [nltk_data] | Downloading package averaged_perceptron_tagger to [nltk_data] | /root/nltk_data... [nltk_data] | Package averaged_perceptron_tagger is already up- [nltk_data] | to-date! [nltk_data] | [nltk_data] Done downloading collection popular . True . Mount your Google Drive, using &#39;google.colab&#39; library. The contents of the drive are available under the folder : &#39;/content/gdrive/My Drive&#39;. We will store our data csv files in the &#39;data/&#39; folder, inside our drive. . from google.colab import drive drive.mount(&#39;/content/gdrive&#39;) . Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(&#34;/content/gdrive&#34;, force_remount=True). . data_location = &#39;/content/gdrive/My Drive/data/consumer_complaints.csv&#39; . def find_files(path): return glob.glob(path) . data_file_list = find_files(data_location) . for file in data_file_list: print(file) . /content/gdrive/My Drive/data/consumer_complaints.csv . Read your CSV File into a Pandas DataFrame object . df = pd.read_csv(data_location) . /usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (5,11) have mixed types. Specify dtype option on import or set low_memory=False. interactivity=interactivity, compiler=compiler, result=result) . Let&#39;s get an over view of our data. . df.head() . date_received product sub_product issue sub_issue consumer_complaint_narrative company_public_response company state zipcode tags consumer_consent_provided submitted_via date_sent_to_company company_response_to_consumer timely_response consumer_disputed? complaint_id . 0 08/30/2013 | Mortgage | Other mortgage | Loan modification,collection,foreclosure | NaN | NaN | NaN | U.S. Bancorp | CA | 95993 | NaN | NaN | Referral | 09/03/2013 | Closed with explanation | Yes | Yes | 511074 | . 1 08/30/2013 | Mortgage | Other mortgage | Loan servicing, payments, escrow account | NaN | NaN | NaN | Wells Fargo &amp; Company | CA | 91104 | NaN | NaN | Referral | 09/03/2013 | Closed with explanation | Yes | Yes | 511080 | . 2 08/30/2013 | Credit reporting | NaN | Incorrect information on credit report | Account status | NaN | NaN | Wells Fargo &amp; Company | NY | 11764 | NaN | NaN | Postal mail | 09/18/2013 | Closed with explanation | Yes | No | 510473 | . 3 08/30/2013 | Student loan | Non-federal student loan | Repaying your loan | Repaying your loan | NaN | NaN | Navient Solutions, Inc. | MD | 21402 | NaN | NaN | Email | 08/30/2013 | Closed with explanation | Yes | Yes | 510326 | . 4 08/30/2013 | Debt collection | Credit card | False statements or representation | Attempted to collect wrong amount | NaN | NaN | Resurgent Capital Services L.P. | GA | 30106 | NaN | NaN | Web | 08/30/2013 | Closed with explanation | Yes | Yes | 511067 | . We will use the &#39;consumer_complaint_narrative&#39; column. For our current purpose, we do not need rows which have no text in that column. Drop all the columns where &#39;consumer_complaint_narrative&#39; column value is absent . cust_complaint_df = df[df[&#39;consumer_complaint_narrative&#39;].notnull()] . cust_complaint_df.head() . date_received product sub_product issue sub_issue consumer_complaint_narrative company_public_response company state zipcode tags consumer_consent_provided submitted_via date_sent_to_company company_response_to_consumer timely_response consumer_disputed? complaint_id . 190126 03/19/2015 | Debt collection | Other (i.e. phone, health club, etc.) | Cont&#39;d attempts collect debt not owed | Debt was paid | XXXX has claimed I owe them {$27.00} for XXXX ... | NaN | Diversified Consultants, Inc. | NY | 121XX | Older American | Consent provided | Web | 03/19/2015 | Closed with explanation | Yes | No | 1290516 | . 190135 03/19/2015 | Consumer Loan | Vehicle loan | Managing the loan or lease | NaN | Due to inconsistencies in the amount owed that... | NaN | M&amp;T Bank Corporation | VA | 221XX | Servicemember | Consent provided | Web | 03/19/2015 | Closed with explanation | Yes | No | 1290492 | . 190155 03/19/2015 | Mortgage | Conventional fixed mortgage | Loan modification,collection,foreclosure | NaN | In XX/XX/XXXX my wages that I earned at my job... | NaN | Wells Fargo &amp; Company | CA | 946XX | NaN | Consent provided | Web | 03/19/2015 | Closed with explanation | Yes | Yes | 1290524 | . 190207 03/19/2015 | Mortgage | Conventional fixed mortgage | Loan servicing, payments, escrow account | NaN | I have an open and current mortgage with Chase... | NaN | JPMorgan Chase &amp; Co. | CA | 900XX | Older American | Consent provided | Web | 03/19/2015 | Closed with explanation | Yes | Yes | 1290253 | . 190208 03/19/2015 | Mortgage | Conventional fixed mortgage | Credit decision / Underwriting | NaN | XXXX was submitted XX/XX/XXXX. At the time I s... | NaN | Rushmore Loan Management Services LLC | CA | 956XX | Older American | Consent provided | Web | 03/19/2015 | Closed with explanation | Yes | Yes | 1292137 | . Let&#39;s view the quality of text in &#39;consumer_complaint_narrative&#39; column of the data frame . sample = cust_complaint_df[cust_complaint_df.index == 190126][[&#39;consumer_complaint_narrative&#39;]].values[0] print(sample) . [&#39;XXXX has claimed I owe them {$27.00} for XXXX years despite the PROOF of PAYMENT I sent them : canceled check and their ownPAID INVOICE for {$27.00}! nThey continue to insist I owe them and collection agencies are after me. nHow can I stop this harassment for a bill I already paid four years ago? n&#39;] . Now, define your pre-processing function. We will perform the following actions: . Convert all upper case letters to lower case | Replace the following characters with spaces [&#39;/&#39;, &#39;(&#39;, &#39;)&#39;, &#39;{&#39;, &#39;}&#39;, &#39;[&#39;,&#39;]&#39;,&#39;|&#39;, &#39;@&#39;, &#39;,&#39;, &#39;;&#39;] | Remove the following characters from the text [&#39;^&#39;, &#39;0-9&#39;, &#39;a-z&#39;, &#39;#&#39;, &#39;+&#39;, &#39;_&#39;] | Remove the masking character &#39;X&#39; from the text | Remove all stop words | Remove all numbers | from nltk.corpus import stopwords CONVERT_TO_SPACE_REGEX = re.compile(&#39;[/(){} [ ] |@,;]&#39;) BAD_CHARACTERS_REGEX = re.compile(&#39;[^0-9a-z #+_]&#39;) STOPWORDS = set(stopwords.words(&#39;english&#39;)) def text_pre_processor(text): text = text.lower() text = CONVERT_TO_SPACE_REGEX.sub(&#39; &#39;, text) text = BAD_CHARACTERS_REGEX.sub(&#39;&#39;, text) text = text.replace(&#39;x&#39;, &#39;&#39;) text = &#39; &#39;.join(word for word in text.split() if word not in STOPWORDS) return text . cust_complaint_df = cust_complaint_df.reset_index(drop=True) cust_complaint_df[&#39;consumer_complaint_narrative&#39;] = cust_complaint_df[&#39;consumer_complaint_narrative&#39;].apply(text_pre_processor) cust_complaint_df[&#39;consumer_complaint_narrative&#39;] = cust_complaint_df[&#39;consumer_complaint_narrative&#39;].str.replace(&#39; d+&#39;, &#39;&#39;) . Let&#39;s view the quality of text after applying the pre-processing to each text datum in the &#39;consumer_complaint_narrative&#39; column . cust_complaint_df.head() . date_received product sub_product issue sub_issue consumer_complaint_narrative company_public_response company state zipcode tags consumer_consent_provided submitted_via date_sent_to_company company_response_to_consumer timely_response consumer_disputed? complaint_id . 0 03/19/2015 | Debt collection | Other (i.e. phone, health club, etc.) | Cont&#39;d attempts collect debt not owed | Debt was paid | claimed owe years despite proof payment sent ... | NaN | Diversified Consultants, Inc. | NY | 121XX | Older American | Consent provided | Web | 03/19/2015 | Closed with explanation | Yes | No | 1290516 | . 1 03/19/2015 | Consumer Loan | Vehicle loan | Managing the loan or lease | NaN | due inconsistencies amount owed told bank amou... | NaN | M&amp;T Bank Corporation | VA | 221XX | Servicemember | Consent provided | Web | 03/19/2015 | Closed with explanation | Yes | No | 1290492 | . 2 03/19/2015 | Mortgage | Conventional fixed mortgage | Loan modification,collection,foreclosure | NaN | wages earned job decreased almost half knew tr... | NaN | Wells Fargo &amp; Company | CA | 946XX | NaN | Consent provided | Web | 03/19/2015 | Closed with explanation | Yes | Yes | 1290524 | . 3 03/19/2015 | Mortgage | Conventional fixed mortgage | Loan servicing, payments, escrow account | NaN | open current mortgage chase bank # chase repor... | NaN | JPMorgan Chase &amp; Co. | CA | 900XX | Older American | Consent provided | Web | 03/19/2015 | Closed with explanation | Yes | Yes | 1290253 | . 4 03/19/2015 | Mortgage | Conventional fixed mortgage | Credit decision / Underwriting | NaN | submitted time submitted complaint dealt rushm... | NaN | Rushmore Loan Management Services LLC | CA | 956XX | Older American | Consent provided | Web | 03/19/2015 | Closed with explanation | Yes | Yes | 1292137 | . Let&#39;s view the quality of text post-processing . sample = cust_complaint_df[cust_complaint_df.index == 0][[&#39;consumer_complaint_narrative&#39;]].values[0] print(sample) . [&#39;claimed owe years despite proof payment sent canceled check ownpaid invoice continue insist owe collection agencies stop harassment bill already paid four years ago&#39;] .",
            "url": "https://vasu014.github.io/backprop-revelations/2020/03/27/nlp-preprocessing.html",
            "relUrl": "/2020/03/27/nlp-preprocessing.html",
            "date": " • Mar 27, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Deadpool Vs Spiderman",
            "content": "Here we are going to use the Fastai deep learning library, which is built upon PyTorch, to build an image classifier to determine if the image supplied is that of Deadpool or Spiderman. We&#39;ll use a pre-trained model, the Resnet34 model. We are going to train it further on a set of Deadpool and Spiderman images, downloaded from the internet. . %reload_ext autoreload %autoreload 2 %matplotlib inline . Next we perform all the necessary imports . from pathlib import Path import os import PIL.Image from fastai.vision import * from fastai.metrics import error_rate . Removing corrupt images . Set up the paths where we have downloaded the images, I have used the &#39;google-images-download&#39; utitlity, which can be easily installed using pip, to download the images that&#39;ll be used in the training / validation sets. . path = Path(&#39;/home/ubuntu/data/train&#39;) . spidey_path = path/&#39;spiderman&#39; deadpool_path = path/&#39;deadpool&#39; . Next, we check for and remove corrupt images in our set of downloads. . def find_broken_img(path): broken_images = [] valid_count = 0 err_count = 0 for pic in os.listdir(path): try: im = PIL.Image.open(f&#39;{path}/{pic}&#39;) im.verify() valid_count += 1 except (IOError, SyntaxError) as e: err_count += 1 broken_images.append(Path(f&#39;{path}/{pic}&#39;)) return valid_count, err_count, broken_images . valid_count, err_count, broken_images = find_broken_img(spidey_path) print(&quot;Valid Spidey Images : {}&quot;.format(valid_count)) print(&quot;Erroneous Images : {}&quot;.format(err_count)) print(&#39;Removing broken spidey files...&#39;) for path in broken_images: os.remove(path) . Valid Spidey Images : 130 Erroneous Images : 0 Removing broken spidey files... . valid_count, err_count, broken_images = find_broken_img(deadpool_path) print(&quot;Valid Deadpool Images : {}&quot;.format(valid_count)) print(&quot;Erroneous Images : {}&quot;.format(err_count)) print(&#39;Removing broken deadpool files...&#39;) for path in broken_images: os.remove(path) . Valid Deadpool Images : 130 Erroneous Images : 0 Removing broken deadpool files... . Exploring the images . Let us see how our images look, here&#39;s a sample file. . img_samples = [] for pic in os.listdir(deadpool_path): img_samples.append(Path(f&#39;{deadpool_path}/{pic}&#39;)) first_sample = img_samples[0] print(&quot;printing first sample &quot; + f&#39;{first_sample}&#39;) image = PIL.Image.open(f&#39;{first_sample}&#39;) display(image) . printing first sample /home/ubuntu/data/train/deadpool/__59a48ed7599d0.jpg . We load our images inside the fastai ImageDataBunch object, which will be passed on to our model for training and validation. . Note : The parameter &#39;ds_tfms&#39; is a little tricky, one of the transformations takes padding_mode=&#39;reflection&#39; by default, which will throw errors down the line. Refer to this thread for more information : https://forums.fast.ai/t/unable-to-change-padding-to-zeros-when-using-rotation-warp-from-get-transforms/27847/4 . tfms = get_transforms() data = ImageDataBunch.from_folder(&#39;/home/ubuntu/data/&#39;, valid_pct=0.2, ds_tfms = tfms, size=224, padding_mode=&#39;zeros&#39;) . data.show_batch(rows=3, figsize=(8,10)) . The Data Bunch object also contains the classes . print(data.classes) . [&#39;deadpool&#39;, &#39;spiderman&#39;] . learner = create_cnn(data, models.resnet34, metrics=error_rate) . learner.model . Sequential( (0): Sequential( (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False) (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU(inplace) (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False) (4): Sequential( (0): BasicBlock( (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace) (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (1): BasicBlock( (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace) (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (2): BasicBlock( (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace) (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (5): Sequential( (0): BasicBlock( (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (downsample): Sequential( (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): BasicBlock( (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (2): BasicBlock( (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (3): BasicBlock( (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (6): Sequential( (0): BasicBlock( (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (downsample): Sequential( (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): BasicBlock( (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (2): BasicBlock( (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (3): BasicBlock( (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (4): BasicBlock( (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (5): BasicBlock( (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (7): Sequential( (0): BasicBlock( (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace) (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (downsample): Sequential( (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): BasicBlock( (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace) (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (2): BasicBlock( (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace) (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) ) (1): Sequential( (0): AdaptiveConcatPool2d( (ap): AdaptiveAvgPool2d(output_size=1) (mp): AdaptiveMaxPool2d(output_size=1) ) (1): Lambda() (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (3): Dropout(p=0.25) (4): Linear(in_features=1024, out_features=512, bias=True) (5): ReLU(inplace) (6): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (7): Dropout(p=0.5) (8): Linear(in_features=512, out_features=2, bias=True) ) ) . learner.fit_one_cycle(6) . Total time: 00:39 epoch train_loss valid_loss error_rate . 1 0.916803 0.571097 0.274510 . 2 0.678668 0.305783 0.176471 . 3 0.516625 0.246507 0.098039 . 4 0.413502 0.245531 0.098039 . 5 0.356973 0.243703 0.078431 . 6 0.320206 0.235454 0.078431 . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; learner.save(&#39;stage-1&#39;) . interp = ClassificationInterpretation.from_learner(learner) losses,idxs = interp.top_losses() len(data.valid_ds)==len(losses)==len(idxs) . True . interp.plot_top_losses(9, figsize=(15,11)) . &lt;/div&gt; .",
            "url": "https://vasu014.github.io/backprop-revelations/2020/03/12/deadpool-spiderman.html",
            "relUrl": "/2020/03/12/deadpool-spiderman.html",
            "date": " • Mar 12, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "I am Vasu Bhardwaj. I’m an experienced product developer. I am comfortable with working on the backend, the frontend and DevOps, depending on the task at hand, with some experience in managing small teams. I completed my B.Tech in Software Engineering from Delhi Technological University. I thrive on caffeine and love to explore the coffee shops everywhere I go. This blog is a means for me to keep track of various things I’m learning on my journey in the tech world. Here is my resume. . . . . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://vasu014.github.io/backprop-revelations/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

}